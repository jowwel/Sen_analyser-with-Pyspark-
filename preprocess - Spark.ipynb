{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import time\n",
    "from collections import Counter\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, RandomizedSearchCV\n",
    "\n",
    "import csv\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import regexp_tokenize, wordpunct_tokenize,blankline_tokenize\n",
    "from nltk import PorterStemmer, LancasterStemmer, SnowballStemmer\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.util import ngrams\n",
    "import re\n",
    "import string\n",
    "from collections import Counter\n",
    "import json\n",
    "import re as regex\n",
    "import xgboost as xgb\n",
    "from sklearn import model_selection, preprocessing\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "#findspark.init(\"C:\\opt\\spark\\spark-2.3.0-bin-hadoop2.7\")\n",
    "import pyspark.sql.types as typ\n",
    "from pyspark.sql import SparkSession"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "sparkSession = SparkSession.builder \\\n",
    "       .master(\"local\") \\\n",
    "       .appName(\"Spark ML\") \\\n",
    "       .getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>cashtag</th>\n",
       "      <th>id</th>\n",
       "      <th>sentiment score</th>\n",
       "      <th>source</th>\n",
       "      <th>spans</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$F</td>\n",
       "      <td>5540055</td>\n",
       "      <td>-0.454</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>Putting on a little $F short</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>$AAPL</td>\n",
       "      <td>10752226</td>\n",
       "      <td>-0.464</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>short some</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>$BAC</td>\n",
       "      <td>10920221</td>\n",
       "      <td>0.445</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>buying opportunity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>$SHOR</td>\n",
       "      <td>12971398</td>\n",
       "      <td>0.661</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>Scaling Up on Long Position</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>$JPM</td>\n",
       "      <td>16142438</td>\n",
       "      <td>-0.763</td>\n",
       "      <td>stocktwits</td>\n",
       "      <td>its time to sell banks</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  cashtag        id  sentiment score      source                         spans\n",
       "0      $F   5540055           -0.454  stocktwits  Putting on a little $F short\n",
       "1   $AAPL  10752226           -0.464  stocktwits                    short some\n",
       "2    $BAC  10920221            0.445  stocktwits            buying opportunity\n",
       "3   $SHOR  12971398            0.661  stocktwits   Scaling Up on Long Position\n",
       "4    $JPM  16142438           -0.763  stocktwits        its time to sell banks"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#read json file into Spark DataFrame\n",
    "#create spark dataframe from Pandas DF\n",
    "df=pd.read_json(\"data/Microblog_Trialdata.json\")\n",
    "def list_sp(row):\n",
    "    idx=row[\"spans\"]\n",
    "    #ch=\"\".join(x for x in idx if x)\n",
    "    ch=' '.join(idx)\n",
    "    row[\"spans\"]=ch\n",
    "    return row\n",
    "df = df.apply(list_sp, axis=1)\n",
    "train_data=sparkSession.createDataFrame(df)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+------------------+---------------+----------+--------------------+\n",
      "|cashtag|                id|sentiment score|    source|               spans|\n",
      "+-------+------------------+---------------+----------+--------------------+\n",
      "|     $F|           5540055|         -0.454|stocktwits|Putting on a litt...|\n",
      "|  $AAPL|          10752226|         -0.464|stocktwits|          short some|\n",
      "|   $BAC|          10920221|          0.445|stocktwits|  buying opportunity|\n",
      "|  $SHOR|          12971398|          0.661|stocktwits|Scaling Up on Lon...|\n",
      "|   $JPM|          16142438|         -0.763|stocktwits|its time to sell ...|\n",
      "|   $LMT|          14073133|          0.627|stocktwits|       Entering long|\n",
      "|   $DNN|          18479024|          0.653|stocktwits|      picked some up|\n",
      "|   $CRK|          34147106|          0.668|stocktwits|time to accumulat...|\n",
      "|   $CRK|          34147107|           0.46|stocktwits|Looking for a str...|\n",
      "|   $CRK|          34147108|          0.403|stocktwits|Very intrigued wi...|\n",
      "|   $CRK|          34147109|            0.0|stocktwits|short2 48 + - ***...|\n",
      "|   $CRK|          34147110|         -0.438|stocktwits|Biggest Market Lo...|\n",
      "|   $CRK|          34147111|         -0.398|stocktwits|$GOOG $GOOGL woul...|\n",
      "|  $SBUX|719890387314335744|          0.483|   twitter| Buying $SBUX on dip|\n",
      "| $GOOGL|708668814427348992|          -0.48|   twitter|is a short below ...|\n",
      "|     $F|           5540056|         -0.454|stocktwits|don't Putting on ...|\n",
      "+-------+------------------+---------------+----------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#extraire les champ de sentiment score et tweets\n",
    "import pyspark.mllib.regression\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "train_data.show()\n",
    "spans=train_data.select(\"sentiment score\",\"spans\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#renommer le champ sentiment score en \"label\"\n",
    "spans=spans.toDF(\"label\",\"spans\")\n",
    "#temp = new.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------------------------------------------------------+\n",
      "|label |spans                                                               |\n",
      "+------+--------------------------------------------------------------------+\n",
      "|-0.454|Putting on a little $F short                                        |\n",
      "|-0.464|short some                                                          |\n",
      "|0.445 |buying opportunity                                                  |\n",
      "|0.661 |Scaling Up on Long Position                                         |\n",
      "|-0.763|its time to sell banks                                              |\n",
      "|0.627 |Entering long                                                       |\n",
      "|0.653 |picked some up                                                      |\n",
      "|0.668 |time to accumulate for a long position far more upside than downside|\n",
      "|0.46  |Looking for a strong bounce Lunchtime rally coming                  |\n",
      "|0.403 |Very intrigued with the technology and growth potential             |\n",
      "|0.0   |short2 48 + - ***worked puts up                                     |\n",
      "|-0.438|Biggest Market Losers                                               |\n",
      "|-0.398|$GOOG $GOOGL would suck                                             |\n",
      "|0.483 |Buying $SBUX on dip                                                 |\n",
      "|-0.48 |is a short below 740 and is overbought                              |\n",
      "|-0.454|don't Putting on a down little $F short                             |\n",
      "+------+--------------------------------------------------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spans.show(truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#remove some additional features(numbers and unctuatuion)\n",
    "import pyspark.ml.feature as ft\n",
    "tokenizer = ft.RegexTokenizer(\n",
    "inputCol='spans',\n",
    "outputCol='tokens',\n",
    "pattern='\\s+|[$,.\\\"]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import col, udf\n",
    "from pyspark.sql.types import IntegerType\n",
    "countTokens = udf(lambda words: len(words), IntegerType())\n",
    "tok = tokenizer \\\n",
    "    .transform(spans) \\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+\n",
      "| label|               spans|              tokens|\n",
      "+------+--------------------+--------------------+\n",
      "|-0.454|Putting on a litt...|[putting, on, a, ...|\n",
      "|-0.464|          short some|       [short, some]|\n",
      "| 0.445|  buying opportunity|[buying, opportun...|\n",
      "| 0.661|Scaling Up on Lon...|[scaling, up, on,...|\n",
      "|-0.763|its time to sell ...|[its, time, to, s...|\n",
      "| 0.627|       Entering long|    [entering, long]|\n",
      "| 0.653|      picked some up|  [picked, some, up]|\n",
      "| 0.668|time to accumulat...|[time, to, accumu...|\n",
      "|  0.46|Looking for a str...|[looking, for, a,...|\n",
      "| 0.403|Very intrigued wi...|[very, intrigued,...|\n",
      "|   0.0|short2 48 + - ***...|[short2, 48, +, -...|\n",
      "|-0.438|Biggest Market Lo...|[biggest, market,...|\n",
      "|-0.398|$GOOG $GOOGL woul...|[goog, googl, wou...|\n",
      "| 0.483| Buying $SBUX on dip|[buying, sbux, on...|\n",
      "| -0.48|is a short below ...|[is, a, short, be...|\n",
      "|-0.454|don't Putting on ...|[don't, putting, ...|\n",
      "+------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "tok.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now remove stopwords from the review(list of words)    \n",
    "from pyspark.ml.feature import StopWordsRemover\n",
    "    \n",
    "remover=StopWordsRemover(inputCol=\"tokens\", outputCol=\"filtered\")\n",
    "filtered_df=remover.transform(tok)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+\n",
      "| label|               spans|              tokens|            filtered|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "|-0.454|Putting on a litt...|[putting, on, a, ...|[putting, little,...|\n",
      "|-0.464|          short some|       [short, some]|             [short]|\n",
      "| 0.445|  buying opportunity|[buying, opportun...|[buying, opportun...|\n",
      "| 0.661|Scaling Up on Lon...|[scaling, up, on,...|[scaling, long, p...|\n",
      "|-0.763|its time to sell ...|[its, time, to, s...| [time, sell, banks]|\n",
      "| 0.627|       Entering long|    [entering, long]|    [entering, long]|\n",
      "| 0.653|      picked some up|  [picked, some, up]|            [picked]|\n",
      "| 0.668|time to accumulat...|[time, to, accumu...|[time, accumulate...|\n",
      "|  0.46|Looking for a str...|[looking, for, a,...|[looking, strong,...|\n",
      "| 0.403|Very intrigued wi...|[very, intrigued,...|[intrigued, techn...|\n",
      "|   0.0|short2 48 + - ***...|[short2, 48, +, -...|[short2, 48, +, -...|\n",
      "|-0.438|Biggest Market Lo...|[biggest, market,...|[biggest, market,...|\n",
      "|-0.398|$GOOG $GOOGL woul...|[goog, googl, wou...| [goog, googl, suck]|\n",
      "| 0.483| Buying $SBUX on dip|[buying, sbux, on...| [buying, sbux, dip]|\n",
      "| -0.48|is a short below ...|[is, a, short, be...|[short, 740, over...|\n",
      "|-0.454|don't Putting on ...|[don't, putting, ...|[putting, little,...|\n",
      "+------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "filtered_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now make 2-gram model\n",
    "from pyspark.ml.feature import NGram\n",
    "\n",
    "ngram=NGram(n=1, inputCol=\"filtered\", outputCol=\"n-gram\")\n",
    "gram_df=ngram.transform(filtered_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "| label|               spans|              tokens|            filtered|              n-gram|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-0.454|Putting on a litt...|[putting, on, a, ...|[putting, little,...|[putting, little,...|\n",
      "|-0.464|          short some|       [short, some]|             [short]|             [short]|\n",
      "| 0.445|  buying opportunity|[buying, opportun...|[buying, opportun...|[buying, opportun...|\n",
      "| 0.661|Scaling Up on Lon...|[scaling, up, on,...|[scaling, long, p...|[scaling, long, p...|\n",
      "|-0.763|its time to sell ...|[its, time, to, s...| [time, sell, banks]| [time, sell, banks]|\n",
      "| 0.627|       Entering long|    [entering, long]|    [entering, long]|    [entering, long]|\n",
      "| 0.653|      picked some up|  [picked, some, up]|            [picked]|            [picked]|\n",
      "| 0.668|time to accumulat...|[time, to, accumu...|[time, accumulate...|[time, accumulate...|\n",
      "|  0.46|Looking for a str...|[looking, for, a,...|[looking, strong,...|[looking, strong,...|\n",
      "| 0.403|Very intrigued wi...|[very, intrigued,...|[intrigued, techn...|[intrigued, techn...|\n",
      "|   0.0|short2 48 + - ***...|[short2, 48, +, -...|[short2, 48, +, -...|[short2, 48, +, -...|\n",
      "|-0.438|Biggest Market Lo...|[biggest, market,...|[biggest, market,...|[biggest, market,...|\n",
      "|-0.398|$GOOG $GOOGL woul...|[goog, googl, wou...| [goog, googl, suck]| [goog, googl, suck]|\n",
      "| 0.483| Buying $SBUX on dip|[buying, sbux, on...| [buying, sbux, dip]| [buying, sbux, dip]|\n",
      "| -0.48|is a short below ...|[is, a, short, be...|[short, 740, over...|[short, 740, over...|\n",
      "|-0.454|don't Putting on ...|[don't, putting, ...|[putting, little,...|[putting, little,...|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "gram_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfrom functools import reduce\\nfrom pyspark.sql.functions import regexp_replace\\nsource_df = sparkSession.createDataFrame(\\n    [\\n        (\"h!o!c!k!e!y\", \"rangers\", \"new york\"),\\n        (\"soccer\", \"??nacional!!\", \"medellin\")\\n    ],\\n    [\"sport\", \"team\", \"city\"]\\n)\\n\\nprint(source_df.show())\\ndef remove_some_chars(col_name):\\n    removed_chars = (\"!\", \"?\")\\n    regexp = \"|\".join(\\'\\\\{0}\\'.format(i) for i in removed_chars)\\n    return regexp_replace(col_name, regexp, \"\")\\n\\nactual_df = (reduce(\\n    lambda memo_df, col_name: memo_df.withColumn(col_name, remove_some_chars(col_name)),\\n    [\"sport\", \"team\"],\\n    source_df\\n))\\n\\nprint(actual_df.show())\\n'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from functools import reduce\n",
    "from pyspark.sql.functions import regexp_replace\n",
    "source_df = sparkSession.createDataFrame(\n",
    "    [\n",
    "        (\"h!o!c!k!e!y\", \"rangers\", \"new york\"),\n",
    "        (\"soccer\", \"??nacional!!\", \"medellin\")\n",
    "    ],\n",
    "    [\"sport\", \"team\", \"city\"]\n",
    ")\n",
    "\n",
    "print(source_df.show())\n",
    "def remove_some_chars(col_name):\n",
    "    removed_chars = (\"!\", \"?\")\n",
    "    regexp = \"|\".join('\\{0}'.format(i) for i in removed_chars)\n",
    "    return regexp_replace(col_name, regexp, \"\")\n",
    "\n",
    "actual_df = (reduce(\n",
    "    lambda memo_df, col_name: memo_df.withColumn(col_name, remove_some_chars(col_name)),\n",
    "    [\"sport\", \"team\"],\n",
    "    source_df\n",
    "))\n",
    "\n",
    "print(actual_df.show())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#now make term frequency vectors out of data frame to feed machine\\nfrom pyspark.ml.feature import HashingTF,IDF\\nhashingtf=HashingTF(inputCol=\"n-gram\", outputCol=\"tf\", numFeatures=100)\\ntf_df=hashingtf.transform(gram_df)\\n   '"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " \"\"\"\n",
    " #now make term frequency vectors out of data frame to feed machine\n",
    "from pyspark.ml.feature import HashingTF,IDF\n",
    "hashingtf=HashingTF(inputCol=\"n-gram\", outputCol=\"tf\", numFeatures=100)\n",
    "tf_df=hashingtf.transform(gram_df)\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntf_df.select(\"tf\").show(tf_df.select(\"tf\").count())\\n'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "tf_df.select(\"tf\").show(tf_df.select(\"tf\").count())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndef square(x):\\n    return x**2\\n'"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "def square(x):\n",
    "    return x**2\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#apply countvectorizer model\n",
    "from pyspark.ml.feature import CountVectorizer\n",
    "from pyspark.ml.feature import StringIndexer\n",
    "\n",
    "# fit a CountVectorizerModel from the corpus.\n",
    "cv = CountVectorizer(inputCol=\"n-gram\", outputCol=\"CV_vector\", minDF=1.0)\n",
    "\n",
    "model = cv.fit(gram_df)\n",
    "\n",
    "result = model.transform(gram_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "#former le pipeline totale\n",
    "from pyspark.ml import Pipeline\n",
    "pipeline = Pipeline(stages=[tokenizer, remover, ngram,cv])\n",
    "pipelineFit = pipeline.fit(spans)\n",
    "final_df = pipelineFit.transform(spans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "| label|               spans|              tokens|            filtered|              n-gram|           CV_vector|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "|-0.454|Putting on a litt...|[putting, on, a, ...|[putting, little,...|[putting, little,...|(44,[0,3,5,6],[1....|\n",
      "|-0.464|          short some|       [short, some]|             [short]|             [short]|      (44,[0],[1.0])|\n",
      "| 0.445|  buying opportunity|[buying, opportun...|[buying, opportun...|[buying, opportun...|(44,[4,18],[1.0,1...|\n",
      "| 0.661|Scaling Up on Lon...|[scaling, up, on,...|[scaling, long, p...|[scaling, long, p...|(44,[1,7,26],[1.0...|\n",
      "|-0.763|its time to sell ...|[its, time, to, s...| [time, sell, banks]| [time, sell, banks]|(44,[2,10,31],[1....|\n",
      "| 0.627|       Entering long|    [entering, long]|    [entering, long]|    [entering, long]|(44,[1,21],[1.0,1...|\n",
      "| 0.653|      picked some up|  [picked, some, up]|            [picked]|            [picked]|     (44,[28],[1.0])|\n",
      "| 0.668|time to accumulat...|[time, to, accumu...|[time, accumulate...|[time, accumulate...|(44,[1,2,7,19,33,...|\n",
      "|  0.46|Looking for a str...|[looking, for, a,...|[looking, strong,...|[looking, strong,...|(44,[12,25,29,30,...|\n",
      "| 0.403|Very intrigued wi...|[very, intrigued,...|[intrigued, techn...|[intrigued, techn...|(44,[8,15,32,42],...|\n",
      "|   0.0|short2 48 + - ***...|[short2, 48, +, -...|[short2, 48, +, -...|[short2, 48, +, -...|(44,[9,11,13,35,3...|\n",
      "|-0.438|Biggest Market Lo...|[biggest, market,...|[biggest, market,...|[biggest, market,...|(44,[20,23,37],[1...|\n",
      "|-0.398|$GOOG $GOOGL woul...|[goog, googl, wou...| [goog, googl, suck]| [goog, googl, suck]|(44,[16,17,24],[1...|\n",
      "| 0.483| Buying $SBUX on dip|[buying, sbux, on...| [buying, sbux, dip]| [buying, sbux, dip]|(44,[4,22,41],[1....|\n",
      "| -0.48|is a short below ...|[is, a, short, be...|[short, 740, over...|[short, 740, over...|(44,[0,14,27],[1....|\n",
      "|-0.454|don't Putting on ...|[don't, putting, ...|[putting, little,...|[putting, little,...|(44,[0,3,5,6],[1....|\n",
      "+------+--------------------+--------------------+--------------------+--------------------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#final_df=result.select(\"label\",\"features\").show(truncate=False)\n",
    "final_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrdd=final_df.rdd\\nbow=rdd.map(lambda x: x[5]).take(final_df.count())\\n'"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rdd=final_df.rdd\n",
    "bow=rdd.map(lambda x: x[5]).take(final_df.count())\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nfinal_df.select(\"features\").take(5)\\nL=[]\\nfor e in bow:\\n    L.append(e)\\nL[0][2]\\n'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "final_df.select(\"features\").take(5)\n",
    "L=[]\n",
    "for e in bow:\n",
    "    L.append(e)\n",
    "L[0][2]\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ndf = sparkSession.createDataFrame(\\n  [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\\n  [\"label\", \"raw\"])\\ncv = CountVectorizer(inputCol=\"n-gram\", outputCol=\"vectors\")\\nmodel = cv.fit(data_ngram)\\nmodel.transform(data_ngram).show(truncate=False)\\n'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "df = sparkSession.createDataFrame(\n",
    "  [(0, [\"a\", \"b\", \"c\"]), (1, [\"a\", \"b\", \"b\", \"c\", \"a\"])],\n",
    "  [\"label\", \"raw\"])\n",
    "cv = CountVectorizer(inputCol=\"n-gram\", outputCol=\"vectors\")\n",
    "model = cv.fit(data_ngram)\n",
    "model.transform(data_ngram).show(truncate=False)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.linalg import Vectors\n",
    "train_df=final_df.select(\"label\",\"CV_vector\")\n",
    "#transformed_df = train_df.rdd.map(lambda row: LabeledPoint(row[0], Vectors.dense(row[1:])))\n",
    "\n",
    "#X_train = train_df.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+------------------------------------------------------+\n",
      "|label |CV_vector                                             |\n",
      "+------+------------------------------------------------------+\n",
      "|-0.454|(44,[0,3,5,6],[1.0,1.0,1.0,1.0])                      |\n",
      "|-0.464|(44,[0],[1.0])                                        |\n",
      "|0.445 |(44,[4,18],[1.0,1.0])                                 |\n",
      "|0.661 |(44,[1,7,26],[1.0,1.0,1.0])                           |\n",
      "|-0.763|(44,[2,10,31],[1.0,1.0,1.0])                          |\n",
      "|0.627 |(44,[1,21],[1.0,1.0])                                 |\n",
      "|0.653 |(44,[28],[1.0])                                       |\n",
      "|0.668 |(44,[1,2,7,19,33,34,40],[1.0,1.0,1.0,1.0,1.0,1.0,1.0])|\n",
      "|0.46  |(44,[12,25,29,30,36,38],[1.0,1.0,1.0,1.0,1.0,1.0])    |\n",
      "|0.403 |(44,[8,15,32,42],[1.0,1.0,1.0,1.0])                   |\n",
      "|0.0   |(44,[9,11,13,35,39,43],[1.0,1.0,1.0,1.0,1.0,1.0])     |\n",
      "|-0.438|(44,[20,23,37],[1.0,1.0,1.0])                         |\n",
      "|-0.398|(44,[16,17,24],[1.0,1.0,1.0])                         |\n",
      "|0.483 |(44,[4,22,41],[1.0,1.0,1.0])                          |\n",
      "|-0.48 |(44,[0,14,27],[1.0,1.0,1.0])                          |\n",
      "|-0.454|(44,[0,3,5,6],[1.0,1.0,1.0,1.0])                      |\n",
      "+------+------------------------------------------------------+\n",
      "\n",
      "root\n",
      " |-- label: double (nullable = true)\n",
      " |-- CV_vector: vector (nullable = true)\n",
      " |-- features: vector (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.ml.feature import VectorAssembler\n",
    "features=[\"CV_vector\"]\n",
    "train_df.show(truncate=False)\n",
    "vectorAssembler = VectorAssembler(inputCols=features, outputCol=\"features\")  \n",
    "transformed = vectorAssembler.transform(train_df)\n",
    "transformed.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "vhouse_df = transformed.select(['features', 'label'])\n",
    "(training, test) = vhouse_df.randomSplit([.7, .3])\n",
    "#X_train = training.rdd.map(lambda line:LabeledPoint(line[0],[line[1:]]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+------+\n",
      "|            features| label|\n",
      "+--------------------+------+\n",
      "|(44,[0,3,5,6],[1....|-0.454|\n",
      "|      (44,[0],[1.0])|-0.464|\n",
      "|(44,[4,18],[1.0,1...| 0.445|\n",
      "|(44,[1,7,26],[1.0...| 0.661|\n",
      "|(44,[2,10,31],[1....|-0.763|\n",
      "|(44,[1,21],[1.0,1...| 0.627|\n",
      "|     (44,[28],[1.0])| 0.653|\n",
      "|(44,[1,2,7,19,33,...| 0.668|\n",
      "|(44,[12,25,29,30,...|  0.46|\n",
      "|(44,[8,15,32,42],...| 0.403|\n",
      "|(44,[9,11,13,35,3...|   0.0|\n",
      "|(44,[20,23,37],[1...|-0.438|\n",
      "|(44,[16,17,24],[1...|-0.398|\n",
      "|(44,[4,22,41],[1....| 0.483|\n",
      "|(44,[0,14,27],[1....| -0.48|\n",
      "|(44,[0,3,5,6],[1....|-0.454|\n",
      "+--------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "vhouse_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\nfrom pyspark.ml.regression import RandomForestRegressor\\n#model = RandomForest.trainRegressor(X_train,{}, numTrees =10,maxDepth =None,maxBins =32, seed=42)\\nRF=RandomForestRegressor(labelCol='label', featuresCol='features',numTrees=2, maxDepth=2, seed=42)\\nmodel = RF.fit(training)\\n\""
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "#model = RandomForest.trainRegressor(X_train,{}, numTrees =10,maxDepth =None,maxBins =32, seed=42)\n",
    "RF=RandomForestRegressor(labelCol='label', featuresCol='features',numTrees=2, maxDepth=2, seed=42)\n",
    "model = RF.fit(training)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression\n",
    "from pyspark.ml.regression import RandomForestRegressor\n",
    "\n",
    "#model = LinearRegression(featuresCol = 'features', labelCol='label', maxIter=10, regParam=0.3, elasticNetParam=0.8)\n",
    "model = RandomForestRegressor(featuresCol = 'features', labelCol='label',numTrees=2, maxDepth=2, seed=42)\n",
    "#lr_model = model.fit(training)\n",
    "rf_model = model.fit(training)\n",
    "#print(\"Coefficients: \" + str(rf_model.coefficients))\n",
    "#print(\"Intercept: \" + str(rf_model.intercept))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#----------------------LinearRegression---------------------\\ntrainingSummary = lr_model.summary\\nprint(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\\nprint(\"r2: %f\" % trainingSummary.r2)\\n'"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#----------------------LinearRegression---------------------\n",
    "trainingSummary = lr_model.summary\n",
    "print(\"RMSE: %f\" % trainingSummary.rootMeanSquaredError)\n",
    "print(\"r2: %f\" % trainingSummary.r2)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+------+--------------------+\n",
      "|         prediction| label|            features|\n",
      "+-------------------+------+--------------------+\n",
      "|-0.2477916666666667|-0.454|(44,[0,3,5,6],[1....|\n",
      "|-0.2477916666666667| 0.403|(44,[8,15,32,42],...|\n",
      "|-0.2477916666666667|-0.398|(44,[16,17,24],[1...|\n",
      "+-------------------+------+--------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#-----------------------RandomForest-----------------\n",
    "rf_predictions = rf_model.transform(test)\n",
    "rf_predictions.select(\"prediction\",\"label\",\"features\").show(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nrdd = sc.textFile(\"text.txt\")\\nrdd.take(5)\\ndef f(lines):\\n    lines = lines.lower()\\n    lines = lines.split(\" \")\\n    return lines\\nrdd1 = rdd.map(f)\\n'"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "rdd = sc.textFile(\"text.txt\")\n",
    "rdd.take(5)\n",
    "def f(lines):\n",
    "    lines = lines.lower()\n",
    "    lines = lines.split(\" \")\n",
    "    return lines\n",
    "rdd1 = rdd.map(f)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nlabel=df.map(lambda x: x[2]).take(5)\\ntweet=df.map(lambda x: x[4]).take(5)\\nprint(label[0])\\ndata=[]\\ni=0\\nfor e in label:\\n    data.append(LabeledPoint(label[i], tweet[i]))\\n    i=i+1\\n'"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "label=df.map(lambda x: x[2]).take(5)\n",
    "tweet=df.map(lambda x: x[4]).take(5)\n",
    "print(label[0])\n",
    "data=[]\n",
    "i=0\n",
    "for e in label:\n",
    "    data.append(LabeledPoint(label[i], tweet[i]))\n",
    "    i=i+1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n#make a labeled point from a DF\\nl2=df.map(lambda row: LabeledPoint(row.label, row.features))\\n#pos = LabeledPoint(label, tweet)\\ndata = [\\n     LabeledPoint(0.0, [0.0]),\\n     LabeledPoint(0.0, [1.0]),\\n     LabeledPoint(1.0, [2.0]),\\n     LabeledPoint(1.0, [3.0])\\n ]\\ndata.append(LabeledPoint(1.0, [3.0]))\\ntype(data)\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\"\"\"\n",
    "#make a labeled point from a DF\n",
    "l2=df.map(lambda row: LabeledPoint(row.label, row.features))\n",
    "#pos = LabeledPoint(label, tweet)\n",
    "data = [\n",
    "     LabeledPoint(0.0, [0.0]),\n",
    "     LabeledPoint(0.0, [1.0]),\n",
    "     LabeledPoint(1.0, [2.0]),\n",
    "     LabeledPoint(1.0, [3.0])\n",
    " ]\n",
    "data.append(LabeledPoint(1.0, [3.0]))\n",
    "type(data)\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
